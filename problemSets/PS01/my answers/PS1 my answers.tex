\documentclass[12pt,letterpaper]{article}
\usepackage{graphicx,textcomp}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{color}
\usepackage[reqno]{amsmath}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{amssymb,enumerate}
\usepackage[all]{xy}
\usepackage{endnotes}
\usepackage{lscape}
\newtheorem{com}{Comment}
\usepackage{float}
\usepackage{hyperref}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
\usepackage[compact]{titlesec}
\usepackage{dcolumn}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{multirow}
\usepackage{xcolor}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\definecolor{light-gray}{gray}{0.65}
\usepackage{url}
\usepackage{listings}
\usepackage{color}
\usepackage{placeins}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}
\newcommand{\Sref}[1]{Section~\ref{#1}}
\newtheorem{hyp}{Hypothesis}

\title{Problem Set 1}
\date{February 11, 2024}
\author{Yajie Dong}





\begin{document}
\maketitle


\section*{Question 1}
\subsection*{Step 1: Understanding the Test Statistic}
The test statistic \( D \) measures the maximum absolute difference between the empirical distribution function(EDF) of the sample and the theoretical cumulative distribution function (CDF) of the hypothesized distribution. This statistic is pivotal in measuring the deviation of the observed data from the expected theoretical model.

\subsection*{Step 2: Calculating \( D \)}
The calculation of  \( D \) involves sorting the sample data, computing the EDF, and identifying the maximum discrepancy between the EDF and the theoretical CDF, as denoted by:
\begin{equation}
D = \max_{i=1:n} \left\{ \frac{i}{n} - F_{(i)}, F_{(i)} - \frac{i-1}{n} \right\}
\end{equation}

where \( F_{(i)} \) represents the value of the theoretical CDF at the \( i \)-th ordered data point.

\vspace{5mm} % Adjust the 5mm as needed for more or less space
\textbf{Now, Let’s calculating it in R:}
\vspace{5mm} % Adjust the 5mm as needed for more or less space

\textbf{Generate Sample Data:}

\begin{lstlisting}[language=R]
# Load necessary library
if (!requireNamespace("stats", quietly = TRUE)) install.packages("stats")
library(stats)

# 1. Set the seed for reproducibility
set.seed(123)

# 2. Generate 1,000 Cauchy random variables
data <- rcauchy(1000, location = 0, scale = 1)
\end{lstlisting}

\textbf{Calculate the Empirical CDF:}

\begin{lstlisting}[language=R]
# 3. Create the empirical CDF from the generated data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
\end{lstlisting}

\textbf{Calculate the Theoretical CDF:}
\vspace{5mm} % Adjust the 5mm as needed for more or less space

It's crucial to align the theoretical CDF with the distribution under test. Here, we assume a standard normal distribution for comparison purposes, although the choice of distribution should be guided by the context of the data or specific research questions.

\begin{lstlisting}[language=R]
# 4. Calculate the theoretical CDF using a normal distribution
# Assuming a standard normal distribution (mean=0, sd=1)
theoreticalCDF <- pnorm(data, mean=0, sd=1)
\end{lstlisting}

\textbf{Calculate the Test Statistic\( D \):}
\begin{lstlisting}[language=R]
# 5. Determine the test statistic D
D <- max(abs(empiricalCDF - theoreticalCDF))

# Display the test statistic
print(paste("Test Statistic D:", D))
\end{lstlisting}

The result we get from R:
\begin{equation}
\text{Test Statistic } D = 0.13472806160635
\end{equation}

\textbf{Interpretation of the test statistic \( D = 0.13472806160635 \):}

Given the test statistic \( D = 0.13472806160635 \), this represents the maximum distance between the empirical Cumulative Distribution Function (CDF) from our sample and the theoretical CDF of a standard normal distribution. This distance highlights how much our observed data differs from the expected under the normal model. The size of \( D \) suggests a significant deviation, potentially indicating a notable difference that, pending the \( p \)-value analysis in the next step, may lead to rejecting the null hypothesis that the distributions are identical.


\subsection*{Step 3: Using \texttt{ks.test()} in R}
The \texttt{ks.test()} function offers a comprehensive approach to performing the Kolmogorov-Smirnov test, yielding both the test statistic and the p-value, thereby facilitating the validation of the manually computed \( D \) statistic.

The p-value is calculated from the Kolmogorov-Smirnoff CDF:

\begin{equation}
p(D \leq d) = \frac{\sqrt{2\pi}}{d} \sum_{k=1}^{\infty} e^{-(2k-1)^{2}\pi^{2}/(8d^{2})}
\end{equation}

Now,  Let’s do that in R:
\begin{lstlisting}[language=R]
# 6. Using ks.test for validation (This is optional and serves as a check)
ks_result <- ks.test(data, "pnorm", mean = 0, sd = 1)

# Display ks.test results
print(ks_result)
\end{lstlisting}
The result we get from R:

\begin{table}[H] % The 'H' specifier forces the table to be placed 'Here'
\centering
\begin{tabular}{ll}
\hline
\textbf{Test} & Asymptotic one-sample Kolmogorov-Smirnov test \\
\hline
\textbf{Data} & data \\
\textbf{D Statistic} & 0.13573 \\
\textbf{p-Value} & \(2.22 \times 10^{-16}\) \\
\textbf{Alternative Hypothesis} & two-sided \\
\hline
\end{tabular}
\caption{Summary of the Kolmogorov-Smirnov test results.}
\label{tab:ks_test}
\end{table}

\textbf{Note:}The minor discrepancy between the manually calculated \( D \) and the \( D \) from \texttt{ks.test()} is likely due to computational nuances but does not affect the overall conclusion.


\subsection*{Step 4: Interpretation and Conclusion}

The small p-value and test statistic \( D \)  suggest a significant divergence from the null hypothesis, implying the observed data's distribution differs notably from the normal model. This outcome aligns with expectations given the Cauchy distribution's distinct properties versus the normal distribution. The choice of Cauchy for this analysis highlights the Kolmogorov-Smirnov test's adeptness at capturing distributional variances, especially between dissimilar distributions, underscoring its value in diverse analytical settings.










\section*{Question 2}

The task is to estimate an Ordinary Least Squares (OLS) regression in R using the Newton-Raphson algorithm, specifically the BFGS method, which is a quasi-Newton method. We are expected to demonstrate that the results obtained using this approach are equivalent to those produced by the lm function in R. Now, let’s do that step-by-step:

\subsection*{Step 1: Data Preparation in R:}

\begin{lstlisting}[language=R]
# Load necessary libraries
library(stats)  # For lm and optim functions

# Data Generation
set.seed(123)
data <- data.frame(x = runif(200, 1, 10))
data$y <- 0 + 2.75 * data$x + rnorm(200, 0, 1.5)
\end{lstlisting}
\subsection*{Step 2: Implement OLS Regression Using \texttt{lm}:}
 Use R's \texttt{lm} function to perform a linear regression with \( y \) as the response variable and \( x \) as the predictor.
Let’s do that in R: 
\begin{lstlisting}[language=R]
# OLS Regression using lm
lm_model <- lm(y ~ x, data = data)
print(summary(lm_model))
\end{lstlisting}
\begin{table}[ht]
\centering
\begin{tabular}{lccccc}
\hline
\textbf{Term} & \textbf{Estimate} & \textbf{Std. Error} & & & \\
\hline
(Intercept) & 0.13919 & 0.25276 & & & \\
x & 2.72670 & 0.04159 & & & \\
\hline
\multicolumn{6}{l}{\textbf{Residuals}} \\
\hline
Min & 1Q & Median & 3Q & Max & \\
-3.1906 & -0.9374 & -0.1665 & 0.8931 & 4.8032 & \\
\hline
\multicolumn{6}{l}{Residual standard error: 1.447 on 198 degrees of freedom} \\
\multicolumn{6}{l}{Multiple R-squared: 0.956, Adjusted R-squared: 0.9557} \\
\multicolumn{6}{l}{F-statistic: 4299 on 1 and 198 DF, p-value: < 2.2e-16} \\
\hline
\end{tabular}
\caption{Summary of OLS Regression Results}
\label{tab:ols_results}
\end{table}
\FloatBarrier % Prevents floats from floating past this point
\textbf{Interpretation Of R output OLS Regression using \texttt{lm}:}

The \texttt{lm} model's coefficients indicate that for every one-unit increase in \( x \), \( y \) increases by approximately 2.727 units, assuming the model is a good fit to the data.

The \( p \)-value for the slope coefficient is \( < 2 \times 10^{-16} \), indicating that the relationship between \( x \) and \( y \) is statistically significant.

The \( R^2 \) value is 0.956, suggesting that approximately 95.6\% of the variability in \( y \) can be explained by \( x \) in this linear model.

\subsection*{Step 3: Implement OLS Regression Using Newton-Raphson (BFGS Method):}

1.we use the \textbf{Residual Sum of Squares (RSS) }to help the BFGS method find the best-fitting regression model by minimizing the differences between the observed data and our model's predictions.

Let't do that in R: 

\begin{lstlisting}[language=R]
# Function to calculate residuals sum of squares (RSS)
rss <- function(beta, data) {
  y_pred <- beta[1] + beta[2] * data$x
  return(sum((data$y - y_pred) ^ 2))
}
\end{lstlisting}

2.Implement OLS Regression Using Newton-Raphson (BFGS Method)in R:
\begin{lstlisting}[language=R]
# Initial parameter estimates for the Newton-Raphson algorithm
initial_params <- c(0, 1)  # Intercept and slope

# OLS Regression using Newton-Raphson (BFGS Method)
optim_result <- optim(par = initial_params, fn = rss, data = data, method = "BFGS")
print(optim_result)
\end{lstlisting}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
\( \text{par}[1] \) & 0.139187 \\
\( \text{par}[2] \) & 2.726699 \\
\hline
\textbf{Statistic} & \textbf{Value} \\
\hline
value & 414.4577 \\
\hline
\textbf{Counts} & \\
\hline
function & 28 \\
gradient & 5 \\
\hline
\textbf{Convergence} & 0 \\
\hline
\textbf{Message} & NULL \\
\hline
\end{tabular}
\caption{Optimization Results from R}
\label{tab:optim_results}
\end{table}
\FloatBarrier % Prevents floats from floating past this point
\textbf{Interpretation Of R output OLS Regression using Newton-Raphson (BFGS Method):}

Optimized Coefficients (using the \texttt{optim} function):
\begin{itemize}
    \item Intercept: \( 0.139187 \)
    \item Slope (Coefficient of \( x \)): \( 2.726699 \)
\end{itemize}
The BFGS method yielded nearly identical coefficients to those produced by the \texttt{lm} function, demonstrating the accuracy and effectiveness of this optimization method for OLS regression.


\subsection*{Step 4: Compare Results:}

Finally,the coefficients from both the \texttt{lm} model and the BFGS optimization are displayed for comparison.

Let't do that in R: 
\begin{lstlisting}[language=R]
# Compare the coefficients
cat("Coefficients from lm:\n")
print(coef(lm_model))
cat("Coefficients from BFGS:\n")
print(optim_result$par)
\end{lstlisting}
\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Method} & \textbf{Intercept} & \textbf{x} \\
\hline
Coefficients from \texttt{lm} & 0.1391874 & 2.7266985 \\
Coefficients from BFGS & 0.139187 & 2.726699 \\
\hline
\end{tabular}
\caption{Comparison of Coefficients from \texttt{lm} and BFGS Optimization}
\label{tab:coeff_comparison}
\end{table}
\textbf{Interpretation:}

The equivalence of the coefficients from both methods confirms that the Newton-Raphson algorithm with the BFGS method is a valid approach for estimating OLS regression parameters, yielding results consistent with the conventional \texttt{lm} function in R. The very close match between the two sets of coefficients (intercept and slope) illustrates that both approaches are robust and reliable for linear regression analysis, at least for this dataset.
\subsection*{Step 5: Conclusion:}
We have successfully demonstrated that the Newton-Raphson algorithm with the BFGS method can be used to estimate the coefficients of an OLS regression model, yielding results equivalent to those obtained from the standard \texttt{lm} function in R.

\end{document}
